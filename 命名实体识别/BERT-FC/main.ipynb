{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置（此处使用AutoDL线上平台）\n",
    "Cuda    12.1\n",
    "\n",
    "GPU     RTX 4090D(24GB) * 1\n",
    "\n",
    "CPU     15 vCPU Intel(R) Xeon(R) Platinum 8474C\n",
    "\n",
    "Python            3.10(ubuntu22.04)\n",
    "\n",
    "PyTorch           2.1.0\n",
    "\n",
    "transformers      4.41.2\n",
    "\n",
    "seqeval           1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertModel, AdamW\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "BATCH_SIZE = 16  # 每个 batch 包含 N 个样本\n",
    "EPOCHS = 3  # 训练 3 个 epoch\n",
    "LEARNING_RATE = 5e-5  # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签种类： 17\n",
      "{'O': 0, 'B-NAME': 1, 'I-NAME': 2, 'B-CONT': 3, 'I-CONT': 4, 'B-EDU': 5, 'I-EDU': 6, 'B-TITLE': 7, 'I-TITLE': 8, 'B-ORG': 9, 'I-ORG': 10, 'B-RACE': 11, 'I-RACE': 12, 'B-PRO': 13, 'I-PRO': 14, 'B-LOC': 15, 'I-LOC': 16}\n",
      "{0: 'O', 1: 'B-NAME', 2: 'I-NAME', 3: 'B-CONT', 4: 'I-CONT', 5: 'B-EDU', 6: 'I-EDU', 7: 'B-TITLE', 8: 'I-TITLE', 9: 'B-ORG', 10: 'I-ORG', 11: 'B-RACE', 12: 'I-RACE', 13: 'B-PRO', 14: 'I-PRO', 15: 'B-LOC', 16: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "# 标签定义\n",
    "LABELS = [\n",
    "    \"O\",\n",
    "    \"B-NAME\", \"I-NAME\", \"B-CONT\", \"I-CONT\", \"B-EDU\", \"I-EDU\", \"B-TITLE\", \"I-TITLE\", \"B-ORG\", \"I-ORG\", \"B-RACE\", \"I-RACE\", \"B-PRO\", \"I-PRO\", \"B-LOC\", \"I-LOC\"\n",
    "]\n",
    "label2id = {label: i for i, label in enumerate(LABELS)}  # 标签到 ID 的映射\n",
    "id2label = {i: label for i, label in enumerate(LABELS)}  # ID 到标签的映射\n",
    "print('标签种类：', len(LABELS))\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数量： 3 ['高', '勇', '：', '男', '，', '中', '国', '国', '籍', '，', '无', '境', '外', '居', '留', '权', '，']\n",
      "训练集句子标签： 3 [1, 2, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "测试集句子数量： 2 ['常', '建', '良', '，', '男', '，']\n",
      "测试集句子标签： 2 [1, 2, 2, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 工具函数，读取数据，并按句子切分\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()  # 读取文件中的所有行\n",
    "\n",
    "    texts, labels = [], []\n",
    "    text, label = [], []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\": # 句子切分处，这里检测是否是空行，我们的语料中使用空行切割句子\n",
    "            if text:\n",
    "                texts.append(text)  # 将一个句子的文本添加到文本列表中\n",
    "                # 将一个句子的标签转换为 ID ，添加到标签列表中\n",
    "                labels.append([label2id[l] for l in label])\n",
    "                text, label = [], []\n",
    "        else: # 非句子切分处，也就是如同“项 B-TITLE”这种形式的行\n",
    "            token, tag = line.strip().split()  # 分割文本和标签\n",
    "            text.append(token)\n",
    "            label.append(tag)\n",
    "\n",
    "    if text:\n",
    "        texts.append(text)\n",
    "        labels.append([label2id[l] for l in label])\n",
    "\n",
    "    return texts, labels  # 返回文本和标签列表，这两个都是二维数组\n",
    "# 读取数据\n",
    "train_texts, train_labels = read_data(\"train.txt\")\n",
    "test_texts, test_labels = read_data(\"test.txt\")\n",
    "\n",
    "print('训练集句子数量：', len(train_texts), train_texts[0])\n",
    "print('训练集句子标签：', len(train_labels), train_labels[0])\n",
    "print('测试集句子数量：', len(test_texts), test_texts[0])\n",
    "print('测试集句子标签：', len(test_labels), test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 管理数据\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # 返回数据集大小\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 这里就是按一个句子，一个句子去处理的\n",
    "        text = self.texts[idx]  # 获取第 idx 个样本（句子）的文本，① 这是一个句子的文本\n",
    "        labels = self.labels[idx]  # 获取第 idx 个样本（句子）的标签，这是一个句子的BIO标签，② 但是使用数字形式标注\n",
    "\n",
    "        # 转换成bert的token格式\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # print(111, text)\n",
    "        # 111 ['高', '勇', '：', '男', '，', '中', '国', '国', '籍', '，', '无', '境', '外', '居', '留', '权', '，']\n",
    "        \n",
    "        # print(222, labels)\n",
    "        # 222 [1, 2, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        # print(333, encoding)\n",
    "        # 该对象有3个属性，但后面会被补充为4个属性\n",
    "        \n",
    "        # input_ids：注意是二维数组，101是<cls>特殊标记，然后不足512，padding到512\n",
    "        # input_ids：tensor([[ 101, 7770, 1235, 8038, 4511, 8024,  704, 1744, 1744, 5093, 8024, 3187, 1862, 1912, 2233, 4522, 3326, 8024,  102,    0,    0,    0,    0,    0...]])\n",
    "        \n",
    "        # token_type_ids：用于标记两个句子，比如一个问题标记为0，答案标记为1\n",
    "        # token_type_ids：tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]])\n",
    "\n",
    "        # attention_mask：设置是否参与注意力机制，cls标记为1，但pad标记为0\n",
    "        # attention_mask：tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0...\n",
    "        \n",
    "        word_ids = encoding.word_ids()  # 获取每个 token 所对应的单词索引\n",
    "        label_ids = []\n",
    "        # previous_word_idx = None\n",
    "        # 关于word_ids\n",
    "        # print(444, word_ids)\n",
    "        # 也是512位，[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None, None, None, None, None, None, None, ....]\n",
    "        # 作用：如果某个子词是原始句子中的一个token，它将返回该单词在原始文本中的索引。\n",
    "        # 而如果子词是由于tokenization过程产生的，则不能直接映射到原始单词（比如，是因为标点符号、特殊字符或单词被切分产生的子词），它会返回None。\n",
    "        # 列表的长度与input_ids相同，意味着每个tokenized的子词都有一个对应的word_id。\n",
    "\n",
    "        # 下面主要用于生成第四种数据，更改后的label数据（前面的三种数据分别是：text、labels、encoding），主要是将原来的label拓展到512维\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # None位置填充位置的标签设为-100，仅仅是一个标记，不要和正式的标签id重合\n",
    "            else:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            # 下面的代码有些冗余\n",
    "            # elif word_idx != previous_word_idx:\n",
    "            #     label_ids.append(labels[word_idx])  # 第一次出现的单词的标签\n",
    "            # else:\n",
    "            #     label_ids.append(\n",
    "            #         labels[word_idx] if word_idx is not None else -100)\n",
    "            # previous_word_idx = word_idx\n",
    "        encoding['labels'] = torch.tensor(label_ids)  # 将标签添加到encoding中，这下encoding就有4个属性了：input_ids、token_type_ids、attention_mask、labels\n",
    "        # print(444, encoding['labels'])\n",
    "        # None的位置都被标记为了-100\n",
    "        # 444 tensor([-100, 1, 2, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100,\n",
    "       \n",
    "        # 返回encoding\n",
    "        # 理解下squeeze：torch.tensor([[1,2,3]]).squeeze(0)，返回tensor([1, 2, 3])，从二维压缩到了一维\n",
    "        # Transformers库的tokenizer，用于存储文本处理后的各种信息，如input_ids、attention_mask等。这些张量可能具有一个额外的维度，通常是批处理维度（即使只有一个样本，也会默认添加一个维度，形状为(1, ...))。\n",
    "        # squeeze(0)：这是PyTorch中的一个操作，用于移除张量中大小为1的维度。在这里，0指定要移除第一个维度。如果val是一个形状为(1, ...,)的张量，val.squeeze(0)就会返回一个形状为(...)的张量，即去掉了那个单元素的批次维度，使得处理单个样本时更加方便。\n",
    "\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "class BertForNER(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertForNER, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            \"bert-base-chinese\")  # 加载预训练的 BERT 模型\n",
    "        self.dropout = nn.Dropout(0.3)  # 添加 dropout 层\n",
    "        self.classifier = nn.Linear(\n",
    "            self.bert.config.hidden_size, num_labels)  # 线性层，承当分类任务，在这里该线性层的输入是768维的，输出是17维的，也就是做17分类\n",
    "        # print(111, self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # 这三个输入的大小都是[2, 512], 也就是输入两个句子，每个句子包含512个单词\n",
    "        # 这两个句子是我划分的batch大小, 因为是在本地尝试运行，所以划分的比较小，一般来说可能是8, 16或者32\n",
    "        # print(1, input_ids, input_ids.shape)\n",
    "        # 1 tensor([[ 101,  122,  130,  ...,    0,    0,    0], [ 101, 7770, 1235,  ...,    0,    0,    0]]) torch.Size([2, 512])\n",
    "        \n",
    "        # print(2, attention_mask, attention_mask.shape)\n",
    "        # 2 tensor([[1, 1, 1,  ..., 0, 0, 0], [1, 1, 1,  ..., 0, 0, 0]]) torch.Size([2, 512])\n",
    "\n",
    "        # print(3, labels, labels.shape)\n",
    "        # 3 tensor([[-100,    0,    0,  ..., -100, -100, -100], [-100,    1,    2,  ..., -100, -100, -100]]) torch.Size([2, 512])\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids, attention_mask=attention_mask)  # 获取 BERT 的输出\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)  # 应用 dropout\n",
    "        logits = self.classifier(sequence_output)  # 获取分类结果\n",
    "        # print(111, logits, logits.shape)\n",
    "        # 这里的输出大小是, [2, 512, 17], 也就是对两个句子, 每个句子都包含512个单词，对每个单词都进行17分类\n",
    "        # 111 tensor([[[-0.9038, -0.6527, -1.1142,  ..., -0.0950,  0.4526,  0.4314],\n",
    "        #    [-0.2693, -0.2534, -0.9892,  ..., -0.2062,  0.5349,  0.3265],\n",
    "        #    [ 0.1514,  0.0874, -0.6728,  ...,  0.1152, -0.0778, -0.0510],\n",
    "        #    ...,\n",
    "        #    [ 0.2086, -0.7756, -0.4723,  ...,  0.1401, -0.9681,  1.4077],\n",
    "        #    [-0.0785, -1.0320, -0.4171,  ...,  0.0846, -0.6143,  1.1130],\n",
    "        #    [ 0.4348, -0.6476, -0.0975,  ...,  0.8670, -0.5920,  1.0222]],\n",
    "\n",
    "        #   [[-0.2587,  0.4648, -0.4417,  ...,  0.1298,  0.5619, -0.0681],\n",
    "        #    [ 0.7163, -0.7920, -0.8147,  ..., -0.3541, -0.7456,  0.7558],\n",
    "        #    [ 0.5359, -0.0739, -0.2890,  ..., -0.3686, -0.3514, -0.1295],\n",
    "        #    ...,\n",
    "        #    [ 0.3715, -0.9656, -0.4320,  ...,  0.2127, -0.6772,  0.7296],\n",
    "        #    [ 0.6422, -1.1092, -0.3876,  ...,  0.2827, -0.6668,  1.0752],\n",
    "        #    [ 0.0343, -0.8732, -0.6937,  ...,  0.5872, -0.6445,  0.7841]]],\n",
    "        #  grad_fn=<ViewBackward0>) torch.Size([2, 512, 17])\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None: # 如果提供了正确标签，则我们就可以对比预测标签和正确标签，从而计算交叉熵损失。如果没有提供标签，则不计算交叉熵损失\n",
    "            loss_fn = nn.CrossEntropyLoss()  # 定义损失函数\n",
    "            # 注释：attention_mask.view(-1) 是一个展平操作，\n",
    "            active_loss = attention_mask.view(-1) == 1  # 只计算attentionmask=1的标签的损失，⚠️但是这里岂不是<cls>也参与损失计算\n",
    "            # print(222, active_loss, active_loss.shape)\n",
    "            # 222 tensor([ True,  True,  True,  ..., False, False, False]) torch.Size([1024]), 这里是将两个512单词的句子，展开成了一个1024单词的序列，也就是展平了\n",
    "\n",
    "            active_logits = logits.view(-1, logits.shape[-1])\n",
    "            # print(333, active_logits, active_logits.shape)\n",
    "            # 这里获取的是1024个单词，每个单词的17分类结果\n",
    "            # 333 tensor([[-0.9038, -0.6527, -1.1142,  ..., -0.0950,  0.4526,  0.4314],\n",
    "            #         [-0.2693, -0.2534, -0.9892,  ..., -0.2062,  0.5349,  0.3265],\n",
    "            # ...\n",
    "            #         [ 0.0343, -0.8732, -0.6937,  ...,  0.5872, -0.6445,  0.7841]],\n",
    "            #        grad_fn=<ViewBackward0>) torch.Size([1024, 17])\n",
    "\n",
    "            # torch.where函数接受三个参数：条件、满足条件时的值、不满足条件时的值。它会根据提供的条件张量（第一个参数）来选择第二个或第三个参数中的值，形成一个新的张量。\n",
    "            # active_loss：这是一个布尔张量，表示哪些位置的token是有效（需要计算损失）的，哪些是无效的（如padding，不需要计算损失）。比如，[True, True, False, True, False]意味着前两个和第四个位置的token是有效数据。\n",
    "            # labels.view(-1)：将原始的标签张量labels展平成一维张量，方便与active_loss进行一一对应。比如，原始的labels可能是(batch_size, max_seq_length)形状，展平后变成一个长列表。\n",
    "            # loss_fn.ignore_index：这是损失函数（如CrossEntropyLoss）的一个属性，指定了一个特殊的值，当标签等于这个值时，对应的样本在损失计算中会被忽略。常用值是-100，但可以根据具体损失函数的配置而变化。\n",
    "            # 这里打印了一下, loss_fn.inore_index是-100\n",
    "            # 感觉这一步稍微有一些多余，因为前面已经在label中按位置填充-100了，⚠️代码需要再优化一下\n",
    "            active_labels = torch.where(\n",
    "                active_loss,\n",
    "                labels.view(-1),\n",
    "                torch.tensor(loss_fn.ignore_index).type_as(labels)\n",
    "            )\n",
    "            # print(444, active_labels, active_labels.shape)\n",
    "            # 444 tensor([-100,    0,    0,  ..., -100, -100, -100]) torch.Size([1024])\n",
    "\n",
    "            # active_logits就是预测值，active_labels就是真实值\n",
    "            loss = loss_fn(active_logits, active_labels)  # 计算损失\n",
    "            # print(555, loss, loss.shape)\n",
    "            # 555 tensor(2.8228, grad_fn=<NllLossBackward0>) torch.Size([])\n",
    "        return loss, logits  # 返回损失和分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的BERT模型和分词器\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试用的数据，仅用于查看输出\n",
    "# NERDataset(train_texts, train_labels, tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数量： 3\n",
      "训练集句子数量： {'input_ids': tensor([ 101, 7770, 1235, 8038, 4511, 8024,  704, 1744, 1744, 5093, 8024, 3187,\n",
      "        1862, 1912, 2233, 4522, 3326, 8024,  102,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([-100,    1,    2,    0,    0,    0,    3,    4,    4,    4,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])}\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "train_dataset = NERDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = NERDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "print('训练集句子数量：', len(train_dataset))\n",
    "print('训练集句子数量：', train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = BertForNER(num_labels=len(LABELS))  # 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据，使用dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # 训练数据加载器\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)  # 测试数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\python\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # AdamW 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(device):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            loss, _ = model(\n",
    "                input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 更新参数\n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 1 == 0:  # 每 10 个 batch 打印一次进度\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{EPOCHS}, Batch {batch_idx + 1}/{len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 评估函数\n",
    "def evaluate(device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for batch_idx, batch in enumerate(test_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            _, logits = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # 只考虑有效的标签\n",
    "        true_labels_batch = [[id2label[l] for l in label if l != -100]\n",
    "                             for label in labels.cpu().numpy()]\n",
    "        pred_labels_batch = [[id2label[p] for p, l in zip(\n",
    "            pred, label) if l != -100] for pred, label in zip(predictions.cpu().numpy(), labels.cpu().numpy())]\n",
    "        true_labels.extend(true_labels_batch)\n",
    "        pred_labels.extend(pred_labels_batch)\n",
    "\n",
    "        print(111, true_labels_batch)\n",
    "        print(222, pred_labels_batch)\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:  # 每 10 个 batch 打印一次进度\n",
    "            print(f\"Evaluating Batch {batch_idx + 1}/{len(test_dataloader)}\")\n",
    "\n",
    "    # 使用 seqeval 计算评价指标\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(classification_report(true_labels, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "def predict(text, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    tokens = tokenizer.tokenize(text)  # 对输入文本进行分词，这里是先拆分成单个汉字，单个汉字的形式，和我们的train语料保持一致\n",
    "    # print(111, text)\n",
    "    # print(222, tokens)\n",
    "    encoding = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )\n",
    "    # print(333, encoding)\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, logits = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(\n",
    "        logits, dim=-1).squeeze().cpu().numpy()  # 获取预测结果\n",
    "    # print(444, predictions)\n",
    "\n",
    "    predicted_labels = [id2label[p] for p in predictions]\n",
    "    # print(555, predicted_labels)\n",
    "    return list(zip(tokens, predicted_labels[1:len(tokens) + 1])) # 这个1是去除cls位置的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Batch 1/1, Loss: 3.061004400253296\n",
      "Epoch 1/3, Average Loss: 3.061004400253296\n",
      "Epoch 2/3, Batch 1/1, Loss: 2.1669387817382812\n",
      "Epoch 2/3, Average Loss: 2.1669387817382812\n",
      "Epoch 3/3, Batch 1/1, Loss: 1.5948504209518433\n",
      "Epoch 3/3, Average Loss: 1.5948504209518433\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 [['B-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'B-EDU', 'I-EDU', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE']] [['B-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'B-EDU', 'I-EDU', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE']]\n",
      "222 [['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TITLE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TITLE', 'O']] [['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TITLE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TITLE', 'O']]\n",
      "Evaluation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         EDU       0.00      0.00      0.00         1\n",
      "        NAME       0.00      0.00      0.00         1\n",
      "         PRO       0.00      0.00      0.00         1\n",
      "       TITLE       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         4\n",
      "   macro avg       0.00      0.00      0.00         4\n",
      "weighted avg       0.00      0.00      0.00         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\python\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# 评估\n",
    "evaluate(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 韦棣华女士曾任教于文华书院\n",
      "222 ['韦', '棣', '华', '女', '士', '曾', '任', '教', '于', '文', '华', '书', '院']\n",
      "333 {'input_ids': tensor([[ 101, 7504, 3479, 1290, 1957, 1894, 3295,  818, 3136,  754, 3152, 1290,\n",
      "          741, 7368,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "444 [ 0  1  2  2  0  0  0  0  0  0  9 10 10 10  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9 10 10  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  9  9  9 10 10  0  0  0  1  2  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  1  2  2  2  0  0  0  0  0  0  0  0  0  9  9  9  9  9\n",
      " 10  0  0  0  0  0  2  2  0  0  0  0  0  0  0  0  0  0  0  9 10 10  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  2\n",
      "  0  0  0  0  0  0  0  0  0  0  9  0  9  9  0 10  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "555 ['O', 'B-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'I-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'B-ORG', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[('韦', 'B-NAME'), ('棣', 'I-NAME'), ('华', 'I-NAME'), ('女', 'O'), ('士', 'O'), ('曾', 'O'), ('任', 'O'), ('教', 'O'), ('于', 'O'), ('文', 'B-ORG'), ('华', 'I-ORG'), ('书', 'I-ORG'), ('院', 'I-ORG')]\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "sample_text = \"韦棣华女士曾任教于文华书院\"\n",
    "prediction = predict(sample_text, device)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
