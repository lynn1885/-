{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置（此处使用AutoDL线上平台）\n",
    "Cuda    12.1\n",
    "\n",
    "GPU     RTX 4090D(24GB) * 1\n",
    "\n",
    "CPU     15 vCPU Intel(R) Xeon(R) Platinum 8474C\n",
    "\n",
    "Python            3.10(ubuntu22.04)\n",
    "\n",
    "PyTorch           2.1.0\n",
    "\n",
    "transformers      4.41.2\n",
    "\n",
    "seqeval           1.2.2\n",
    "\n",
    "pytorch-crf       0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertModel, AdamW\n",
    "from seqeval.metrics import classification_report # 可以查看这个教程，https://www.cnblogs.com/jclian91/p/12459688.html\n",
    "from torchcrf import CRF # 这里使用 pytorch-crf 的实现 https://pytorch-crf.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "BATCH_SIZE = 16  # 每个 batch 包含 N 个样本\n",
    "EPOCHS = 3  # 训练 3 个 epoch\n",
    "LEARNING_RATE = 5e-5  # 学习率\n",
    "\n",
    "MAX_SEQ_LEN = 512 # 可处理的最大句子长度，一般取决于预训练模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签种类： 17\n",
      "{'O': 0, 'B-NAME': 1, 'I-NAME': 2, 'B-CONT': 3, 'I-CONT': 4, 'B-EDU': 5, 'I-EDU': 6, 'B-TITLE': 7, 'I-TITLE': 8, 'B-ORG': 9, 'I-ORG': 10, 'B-RACE': 11, 'I-RACE': 12, 'B-PRO': 13, 'I-PRO': 14, 'B-LOC': 15, 'I-LOC': 16, 'NONE': 17}\n",
      "{0: 'O', 1: 'B-NAME', 2: 'I-NAME', 3: 'B-CONT', 4: 'I-CONT', 5: 'B-EDU', 6: 'I-EDU', 7: 'B-TITLE', 8: 'I-TITLE', 9: 'B-ORG', 10: 'I-ORG', 11: 'B-RACE', 12: 'I-RACE', 13: 'B-PRO', 14: 'I-PRO', 15: 'B-LOC', 16: 'I-LOC', 17: 'NONE'}\n"
     ]
    }
   ],
   "source": [
    "# 标签定义\n",
    "LABELS = [\n",
    "    \"O\", \"B-NAME\", \"I-NAME\", \"B-CONT\", \"I-CONT\", \"B-EDU\", \"I-EDU\", \"B-TITLE\", \"I-TITLE\", \"B-ORG\", \"I-ORG\", \"B-RACE\", \"I-RACE\", \"B-PRO\", \"I-PRO\", \"B-LOC\", \"I-LOC\", \"NONE\"\n",
    "] # 这里我们加入了一个NONE标签\n",
    "\n",
    "LABELS_NUM = len(LABELS) - 1 # 分类数量，也就是我们这里是一个几分类问题，NONE不是一个要训练要学习的分类，是一个特殊标记，所以要减掉1，感觉这个时限不太优雅，后期需要修改\n",
    "NONE_LABEL_IDX = len(LABELS) - 1 #这里是个数字记录了，我们设置的NONE标签的下标，NONE标签是荣放在最后一位\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABELS)}  # 标签到 ID 的映射\n",
    "id2label = {i: label for i, label in enumerate(LABELS)}  # ID 到标签的映射\n",
    "print('标签种类：', LABELS_NUM)\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数量： 3 ['高', '勇', '：', '男', '，', '中', '国', '国', '籍', '，', '无', '境', '外', '居', '留', '权', '，']\n",
      "训练集句子标签： 3 [1, 2, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "测试集句子数量： 2 ['常', '建', '良', '，', '男', '，']\n",
      "测试集句子标签： 2 [1, 2, 2, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 工具函数，读取数据，并按句子切分\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()  # 读取文件中的所有行\n",
    "\n",
    "    texts, labels = [], []\n",
    "    text, label = [], []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\": # 句子切分处，这里检测是否是空行，我们的语料中使用空行切割句子，其实也可以替换为\\n\\n\n",
    "            if text:\n",
    "                texts.append(text)  # 将一个句子的文本添加到文本列表中\n",
    "                # 将一个句子的标签转换为 ID ，添加到标签列表中\n",
    "                labels.append([label2id[l] for l in label])\n",
    "                text, label = [], []\n",
    "        else: # 非句子切分处，也就是如同“项 B-TITLE”这种形式的，带有标注的行\n",
    "            token, tag = line.strip().split()  # 分割文本和标签，split默认使用空格切分，例如“学 B-EDU”会切分为“学”“B-EDU”\n",
    "            text.append(token)\n",
    "            label.append(tag)\n",
    "\n",
    "    if text:\n",
    "        texts.append(text)\n",
    "        labels.append([label2id[l] for l in label])\n",
    "\n",
    "    return texts, labels  # 返回文本和标签列表，这两个都是二维数组\n",
    "# 读取数据\n",
    "train_texts, train_labels = read_data(\"train.txt\")\n",
    "test_texts, test_labels = read_data(\"test.txt\")\n",
    "\n",
    "print('训练集句子数量：', len(train_texts), train_texts[0])\n",
    "print('训练集句子标签：', len(train_labels), train_labels[0])\n",
    "print('测试集句子数量：', len(test_texts), test_texts[0])\n",
    "print('测试集句子标签：', len(test_labels), test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 管理数据\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # 返回数据集大小\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 这里就是按一个句子，一个句子去处理的\n",
    "        text = self.texts[idx]  # 获取第 idx 个样本（句子）的文本，① 这是一个句子的文本\n",
    "        labels = self.labels[idx]  # 获取第 idx 个样本（句子）的标签，这是一个句子的BIO标签，② 但是使用数字形式标注\n",
    "\n",
    "        # 转换成bert的token格式\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # print(111, text)\n",
    "        # 111 ['高', '勇', '：', '男', '，', '中', '国', '国', '籍', '，', '无', '境', '外', '居', '留', '权', '，']\n",
    "        \n",
    "        # print(222, labels)\n",
    "        # 222 [1, 2, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        # print(333, encoding)\n",
    "        # encoding该对象有3个属性，但后面会被补充为4个属性\n",
    "        # 这3个属性分别是\n",
    "        # input_ids：注意是二维数组，101是<cls>特殊标记，然后不足512，padding到512\n",
    "        # input_ids：tensor([[ 101, 7770, 1235, 8038, 4511, 8024,  704, 1744, 1744, 5093, 8024, 3187, 1862, 1912, 2233, 4522, 3326, 8024,  102,    0,    0,    0,    0,    0...]])\n",
    "        \n",
    "        # token_type_ids：用于标记两个句子，比如一个问题标记为0，答案标记为1，我们这里都标记为0\n",
    "        # token_type_ids：tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]])\n",
    "\n",
    "        # attention_mask：设置是否参与注意力机制，注意cls位置标记为了1，但pad标记为0\n",
    "        # attention_mask：tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0...\n",
    "\n",
    "        \n",
    "        word_ids = encoding.word_ids()  # 获取每个 token 所对应的单词索引\n",
    "        label_ids = []\n",
    "        # 关于word_ids\n",
    "        # print(444, word_ids)\n",
    "        # 也是512位，[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None, None, None, None, None, None, None, ....]\n",
    "        # 作用：如果某个子词是原始句子中的一个token，它将返回该单词在原始文本中的索引。\n",
    "        # 而如果子词是由于tokenization过程产生的，其无法直接映射到原始单词（比如因为标点符号、特殊字符或单词被切分产生的子词），它会返回None。其实这个在处理英文介词时更为常用\n",
    "        # word_ids列表的长度与input_ids相同，意味着每个tokenized的子词都有一个对应的word_id。\n",
    "\n",
    "        # 下面主要用于生成第四种数据，更改后的label数据（前面的三种数据分别是：text、labels、encoding），主要是将原来的label拓展到512维\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(0)  # 设置word_ids = None的位置填充填充的标签，仅仅是一个标记，不要和正式的标签id重合。但是搭配CRF使用时，我暂且把这里设置维0，否则运算会报错 ⚠️ 这主要是因为这个代码从bert-Softmax修改而来，那个模型中使用-100作为标记，其实实现bert-bilstm-crf时可以尝试换一个方案\n",
    "            else:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            # 下面的代码有些冗余\n",
    "            # elif word_idx != previous_word_idx:\n",
    "            #     label_ids.append(labels[word_idx])  # 第一次出现的单词的标签\n",
    "            # else:\n",
    "            #     label_ids.append(\n",
    "            #         labels[word_idx] if word_idx is not None else -100)\n",
    "            # previous_word_idx = word_idx\n",
    "        encoding['labels'] = torch.tensor(label_ids)  # 将标签添加到encoding中，这下encoding就有4个属性了：input_ids、token_type_ids、attention_mask、labels\n",
    "        # print(444, encoding['labels'])\n",
    "        # None的位置都被标记为了17\n",
    "        # 444 tensor(tensor([17,  1,  2,  0,  0,  0,  3,  4,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0, 17, 17, 17, 17, 17, 17, 17, 17, \n",
    "       \n",
    "        # 返回encoding\n",
    "        # 理解下squeeze：torch.tensor([[1,2,3]]).squeeze(0)，返回tensor([1, 2, 3])，从二维压缩到了一维\n",
    "        # Transformers库的tokenizer，用于存储文本处理后的各种信息，如input_ids、attention_mask等。这些张量可能具有一个额外的维度，通常是批处理维度（即使只有一个样本，也会默认添加一个维度，形状为(1, ...))。\n",
    "        # squeeze(0)：这是PyTorch中的一个操作，用于移除张量中大小为1的维度。在这里，0指定要移除第一个维度。如果val是一个形状为(1, ...,)的张量，val.squeeze(0)就会返回一个形状为(...)的张量，即去掉了那个单元素的批次维度，使得处理单个样本时更加方便。\n",
    "        # 返回的就是一个对象{input_ids, token_type_ids, attention_mask, labels}, 每个属性对应的都是一个512维向量\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "class BertForNER(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertForNER, self).__init__()\n",
    "        \n",
    "        # BERT\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm_hiden = 128 # 隐层神经元个数\n",
    "        self.bilstm = nn.LSTM(\n",
    "            768, # 输入维度\n",
    "            self.lstm_hiden, # 隐层神经元个数\n",
    "            1, # LSTM层数\n",
    "            bidirectional=True, # 双向LSTM\n",
    "            batch_first=True, # batch_first: 如果为 True，则输入和输出的形状为 (batch_size, seq_len, feature_dim)，否则为 (seq_len, batch_size, feature_dim)。\n",
    "            dropout=0.1 # 随机失活\n",
    "        )\n",
    "\n",
    "        # LINEAR，也有说法说，这个线性层不是必要的，需要仔细研究一下CRF的输入\n",
    "        self.linear = nn.Linear(self.lstm_hiden * 2, LABELS_NUM) # 将LSTM的输出降维到标签个数，用于之后分类\n",
    "\n",
    "        # CRF\n",
    "        self.crf = CRF(LABELS_NUM, batch_first=True) # 创建模型：model = CRF(num_tags)  https://pytorch-crf.readthedocs.io/en/stable/\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # BERT\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # seq_out = bert_output[0]  # [batchsize, max_len, 768]    # 尝试一下取last_hidden_state\n",
    "        seq_out = bert_output.last_hidden_state  # [batchsize, max_len, 768]    # 尝试一下取last_hidden_state\n",
    "        batch_size = seq_out.size(0)\n",
    "\n",
    "        # LSTM\n",
    "        seq_out, _ = self.bilstm(seq_out)\n",
    "        seq_out = seq_out.contiguous().view(-1, self.lstm_hiden * 2)\n",
    "        seq_out = seq_out.contiguous().view(batch_size, MAX_SEQ_LEN, -1)\n",
    "        \n",
    "        # LINEAR\n",
    "        seq_out = self.linear(seq_out)\n",
    "        # print(111, seq_out.shape, seq_out)\n",
    "        # 下面的维度“2”，代表两个句子，这个取决于我们设置的batch_size\n",
    "        # 512是每个句子包含512个单词，如果不够或者过长的话，会截长补短\n",
    "        # 17是每个单词进行17分类，这里存储的就是每个单词分配给每个类的概率\n",
    "        # 111 torch.Size([2, 512, 17]) tensor([[[ 0.1956, -0.0638,  0.1322,  ..., -0.0259, -0.0147, -0.0049],\n",
    "        #  [ 0.1993,  0.2236,  0.2785,  ..., -0.0203, -0.0703,  0.1671],\n",
    "        #  [ 0.1984,  0.2107,  0.0458,  ...,  0.0763, -0.0182,  0.1876],\n",
    "\n",
    "        # CRF\n",
    "        # crf的输出结果是一个二维数组\n",
    "        # 其中是每个数组对应一个句子\n",
    "        # 数组中的元素就是这个句子当中的单词最可能得分类\n",
    "        # 注意再考虑句子中的单词时，只考虑Attention mask = 1的，不过在我们这里<cls>也被标记为1\n",
    "        # 222 [[2, 2, 0, 10, 1, 16, 0, 15, 15, 1, 13, 10, 13, 16, 13, 10, 1, 13, 2, 2], [14, 1, 13, 10, 0, 10, 1, 10, 1, 14, 1, 4, 4, 10, 1, 4, 4, 10, 1]]\n",
    "        logits = self.crf.decode(seq_out, mask=attention_mask.bool()) # 这个mask用于忽略掉填充进来的，比如pad之类的信息\n",
    "        # print(222, logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None: # 如果传入的有标签，此时我们应该是在进行训练，使用的是带标签数据，而不是在进行预测\n",
    "            loss = -self.crf(seq_out, labels,\n",
    "                             mask=attention_mask.bool(), reduction='mean')\n",
    "        return loss, logits  # 返回损失和分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的BERT模型和分词器\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试用的数据，仅用于查看输出\n",
    "# NERDataset(train_texts, train_labels, tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数量： 3\n",
      "训练集第一个句子： {'input_ids': tensor([ 101, 7770, 1235, 8038, 4511, 8024,  704, 1744, 1744, 5093, 8024, 3187,\n",
      "        1862, 1912, 2233, 4522, 3326, 8024,  102,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 1, 2, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# 数据加载\n",
    "train_dataset = NERDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = NERDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "print('训练集句子数量：', len(train_dataset))\n",
    "print('训练集第一个句子：', train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\softwares\\python\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = BertForNER(num_labels=LABELS_NUM)  # 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据，使用dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # 训练数据加载器\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)  # 测试数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\python\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # AdamW 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(device):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # 注意这里每一个batch，因为我们设置batch-size=2，所以这里包含两个句子，每个句子的input_ids, labels等属性都是512维\n",
    "            # print(111, batch['input_ids'].shape, batch['input_ids'])\n",
    "            # print(222, batch['token_type_ids'].shape, batch['token_type_ids'])\n",
    "            # print(333, batch['attention_mask'].shape, batch['attention_mask'])\n",
    "            # print(444, batch['labels'].shape, batch['labels'])\n",
    "            # 111 torch.Size([2, 512]) tensor([[101, 123, 121,  ...,   0,   0,   0],\n",
    "            #         [101, 122, 130,  ...,   0,   0,   0]])\n",
    "            # 222 torch.Size([2, 512]) tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
    "            #         [0, 0, 0,  ..., 0, 0, 0]])\n",
    "            # 333 torch.Size([2, 512]) tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
    "            #         [1, 1, 1,  ..., 0, 0, 0]])\n",
    "            # 444 torch.Size([2, 512]) tensor([[17,  0,  0,  ..., 17, 17, 17],\n",
    "            #         [17,  0,  0,  ..., 17, 17, 17]])\n",
    "            loss, _ = model(\n",
    "                input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 更新参数\n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 1 == 0:  # 每 10 个 batch 打印一次进度\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{EPOCHS}, Batch {batch_idx + 1}/{len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 评估函数\n",
    "def evaluate(device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    # 评估也是分batch运行的\n",
    "    # 只是后面生成评估报告，真正计算f1时，需要综合各个batch的结果\n",
    "    # 综合各个batch的内容就放在下面两个数组中\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for batch_idx, batch in enumerate(test_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            _, logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # 下面是获取真值标签和预测标签\n",
    "        # 并且获取到到的都是id形式，需要转换回原来的标签形式\n",
    "        true_labels_batch = []\n",
    "        pred_labels_batch = []\n",
    "        for i in range(len(logits)):\n",
    "            true_labels_batch_ids = labels[i][:len(logits[i])].tolist() # 截断label，因为我们这里的label，按前面的逻辑是512位，但是crf输出的logits，按前面的逻辑只包含Attention_mask为1的部分\n",
    "            true_labels_batch.append([id2label[id] for id in true_labels_batch_ids])\n",
    "            pred_labels_batch.append([id2label[id] for id in logits[i]])\n",
    "        # print(111, len(true_labels_batch))\n",
    "        # print(222, len(pred_labels_batch))\n",
    "  \n",
    "        true_labels.extend(true_labels_batch)\n",
    "        pred_labels.extend(pred_labels_batch)\n",
    "\n",
    "        # 只考虑有效的标签\n",
    "        # true_labels_batch = [[id2label[l] for l in label if l != 17]\n",
    "        #                      for label in labels.cpu().numpy()]\n",
    "        # pred_labels_batch = [[id2label[p] for p, l in zip(\n",
    "        #     pred, label) if l != 17] for pred, label in zip(predictions.cpu().numpy(), labels.cpu().numpy())]\n",
    "        # # print(111, true_labels_batch)\n",
    "        # # print(222, pred_labels_batch)\n",
    "        # true_labels.extend(true_labels_batch)\n",
    "        # pred_labels.extend(pred_labels_batch)\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:  # 每 10 个 batch 打印一次进度\n",
    "            print(f\"Evaluating Batch {batch_idx + 1}/{len(test_dataloader)}\")\n",
    "\n",
    "    # 使用 seqeval 计算评价指标\n",
    "    print('参与评估的句子数量：', len(true_labels), len(pred_labels))\n",
    "    print(\"Evaluation Results:\")\n",
    "    # true_labels、pred_labels是格式、大小一样的两个数组，都是二维数组，一个是真值，一个是预测值\n",
    "    # [['O', 'B-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'B-EDU', 'I-EDU', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE', 'O']]\n",
    "    # [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
    "    print(true_labels)\n",
    "    print(pred_labels)\n",
    "    print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "def predict(text, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    tokens = tokenizer.tokenize(text)  # 对输入文本进行分词，这里是先拆分成单个汉字，单个汉字的形式，和我们的train语料保持一致\n",
    "    # print(111, text)\n",
    "    # print(222, tokens)\n",
    "    encoding = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )\n",
    "    # print(333, encoding)\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, logits = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.tensor(logits)\n",
    "    # print(444, predictions[0])\n",
    "\n",
    "    predicted_labels = [id2label[p] for p in predictions[0].tolist()]\n",
    "    # print(555, predicted_labels)\n",
    "    return list(zip(tokens, predicted_labels[1:len(tokens) + 1])) # 这个1是去除cls位置的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Batch 1/2, Loss: 127.58329010009766\n",
      "Epoch 1/3, Batch 2/2, Loss: 52.27286911010742\n",
      "Epoch 1/3, Average Loss: 89.92807960510254\n",
      "Epoch 2/3, Batch 1/2, Loss: 46.042816162109375\n",
      "Epoch 2/3, Batch 2/2, Loss: 158.37928771972656\n",
      "Epoch 2/3, Average Loss: 102.21105194091797\n",
      "Epoch 3/3, Batch 1/2, Loss: 92.01494598388672\n",
      "Epoch 3/3, Batch 2/2, Loss: 37.671669006347656\n",
      "Epoch 3/3, Average Loss: 64.84330749511719\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参与评估的句子数量： 2 2\n",
      "Evaluation Results:\n",
      "[['O', 'B-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'B-EDU', 'I-EDU', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE', 'I-TITLE', 'O']]\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         EDU       0.00      0.00      0.00         1\n",
      "        NAME       0.00      0.00      0.00         1\n",
      "         PRO       0.00      0.00      0.00         1\n",
      "       TITLE       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         4\n",
      "   macro avg       0.00      0.00      0.00         4\n",
      "weighted avg       0.00      0.00      0.00         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\python\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\softwares\\python\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# 评估\n",
    "evaluate(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('韦', 'O'), ('棣', 'O'), ('华', 'O'), ('女', 'O'), ('士', 'O'), ('曾', 'O'), ('任', 'O'), ('教', 'O'), ('于', 'O'), ('文', 'O'), ('华', 'O'), ('书', 'O'), ('院', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "sample_text = \"韦棣华女士曾任教于文华书院\"\n",
    "prediction = predict(sample_text, device)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
