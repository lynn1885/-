{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入\n",
    "import os\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer\n",
    "from config import args\n",
    "\n",
    "import preprocess\n",
    "import dataset\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日志\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于训练函数的封装\n",
    "class Trainer:\n",
    "    def __init__(self, args, train_loader, dev_loader, test_loader):\n",
    "        self.args = args\n",
    "\n",
    "        # 我把这里注释了，换一种设置device的方式\n",
    "        # gpu_ids = args.gpu_ids.split(',')\n",
    "        # self.device = torch.device(\n",
    "        #     \"cpu\" if gpu_ids[0] == '-1' else \"cuda:\" + gpu_ids[0])\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # 创建模型并移动到设备上\n",
    "        self.model = models.BertForMultiLabelClassification(args)\n",
    "        self.model.to(self.device)\n",
    "        # print(999, self.model)\n",
    "        # 这个就是非常简单，把bert 768维输出，输入到线性层，转换为65个类型的多分类任务\n",
    "        # 999 BertForMultiLabelClassification(\n",
    "        #   (bert): BertModel(\n",
    "        #     (embeddings): BertEmbeddings(\n",
    "        #       (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
    "        #       (position_embeddings): Embedding(512, 768)\n",
    "        #       (token_type_embeddings): Embedding(2, 768)\n",
    "        #       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "        #       (dropout): Dropout(p=0.1, inplace=False)\n",
    "        #     )\n",
    "        #     (encoder): BertEncoder(\n",
    "        #       (layer): ModuleList(\n",
    "        #         (0-11): 12 x BertLayer(\n",
    "        #           (attention): BertAttention(\n",
    "        #             (self): BertSelfAttention(\n",
    "        #               (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "        #               (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "        #               (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "        #               (dropout): Dropout(p=0.1, inplace=False)\n",
    "        #             )\n",
    "        #             (output): BertSelfOutput(\n",
    "        #               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "        #               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "        #               (dropout): Dropout(p=0.1, inplace=False)\n",
    "        #             )\n",
    "        #           )\n",
    "        #           (intermediate): BertIntermediate(\n",
    "        #             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        #             (intermediate_act_fn): GELUActivation()\n",
    "        #           )\n",
    "        #           (output): BertOutput(\n",
    "        #             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        #             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "        #             (dropout): Dropout(p=0.1, inplace=False)\n",
    "        #           )\n",
    "        #         )\n",
    "        #       )\n",
    "        #     )\n",
    "        #     (pooler): BertPooler(\n",
    "        #       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "        #       (activation): Tanh()\n",
    "        #     )\n",
    "        #   )\n",
    "        #   (dropout): Dropout(p=0.3, inplace=False)\n",
    "        #   (linear): Linear(in_features=768, out_features=65, bias=True)\n",
    "        # )\n",
    "\n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            params=self.model.parameters(), lr=self.args.lr)\n",
    "\n",
    "        # 损失函数\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # 数据\n",
    "        self.train_loader = train_loader\n",
    "        self.dev_loader = dev_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    # 从本地加载模型，这个cpk是checkpoint的意思，训练好的模型会保存在这个目录中\n",
    "    def load_ckp(self, model, optimizer, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        return model, optimizer, epoch, loss\n",
    "\n",
    "    # 保存模型到本地\n",
    "    def save_ckp(self, state, checkpoint_path):\n",
    "        torch.save(state, checkpoint_path)\n",
    "\n",
    "    \"\"\"\n",
    "    def save_ckp(self, state, is_best, checkpoint_path, best_model_path):\n",
    "        tmp_checkpoint_path = checkpoint_path\n",
    "        torch.save(state, tmp_checkpoint_path)\n",
    "        if is_best:\n",
    "            tmp_best_model_path = best_model_path\n",
    "            shutil.copyfile(tmp_checkpoint_path, tmp_best_model_path)\n",
    "    \"\"\"\n",
    "\n",
    "    # 训练函数\n",
    "    def train(self):\n",
    "        # 总训练步数\n",
    "        total_step = len(self.train_loader) * self.args.train_epochs\n",
    "        global_step = 0\n",
    "        eval_step = 100  # 这里是每100步做一次评估\n",
    "        best_dev_micro_f1 = 0.0\n",
    "\n",
    "        # 开始训练\n",
    "        for epoch in range(args.train_epochs):\n",
    "            for train_step, train_data in enumerate(self.train_loader):\n",
    "                self.model.train()  # 让模型处于训练状态\n",
    "\n",
    "                # 数据转移到设备上\n",
    "                token_ids = train_data['token_ids'].to(self.device)\n",
    "                attention_masks = train_data['attention_masks'].to(self.device)\n",
    "                token_type_ids = train_data['token_type_ids'].to(self.device)\n",
    "                labels = train_data['labels'].to(self.device)\n",
    "                # print(222, labels.shape, labels)\n",
    "                # 222 torch.Size([32, 65]) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "                # [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "                # [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "                # ...,\n",
    "                # [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "                # [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "                # [0., 0., 0.,  ..., 0., 1., 0.]])\n",
    "\n",
    "                # 获取模型预测\n",
    "                train_outputs = self.model(\n",
    "                    token_ids, attention_masks, token_type_ids)\n",
    "                # print(333, train_outputs.shape, train_outputs)\n",
    "                # 32个样本，每个样本有65维的分类预测\n",
    "                #  333 torch.Size([32, 65]) tensor([[-0.3871, -0.3241, -0.0690,  ..., -0.4411, -0.5412, -0.5385],\n",
    "                # [-0.6295,  0.0456, -0.3547,  ..., -0.0591, -0.2377, -0.7483],\n",
    "                # [-0.1751, -0.4477, -0.5056,  ..., -0.0935, -0.7540, -1.1031],\n",
    "                # ...,\n",
    "                # [-0.5838, -0.8496, -0.0425,  ..., -0.0125, -0.4996, -1.1216],\n",
    "                # [-1.0604, -0.1496, -0.6887,  ..., -0.2512, -0.3959, -0.7717],\n",
    "                # [-0.5345,  0.1968, -0.3468,  ..., -0.9787,  0.2667,  0.2490]],\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.criterion(train_outputs, labels)\n",
    "\n",
    "                # 反向传播\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                logger.info(\n",
    "                    \"【train】 epoch：{} step:{}/{} loss：{:.6f}\".format(epoch, global_step, total_step, loss.item()))\n",
    "                global_step += 1\n",
    "                # 进入dev评估\n",
    "                if global_step % eval_step == 0:\n",
    "                    dev_loss, dev_outputs, dev_targets = self.dev()\n",
    "                    accuracy, micro_f1, macro_f1 = self.get_metrics(\n",
    "                        dev_outputs, dev_targets)\n",
    "                    logger.info(\n",
    "                        \"【dev】 loss：{:.6f} accuracy：{:.4f} micro_f1：{:.4f} macro_f1：{:.4f}\".format(dev_loss, accuracy,\n",
    "                                                                                                   micro_f1, macro_f1))\n",
    "                    if macro_f1 > best_dev_micro_f1:\n",
    "                        logger.info(\"====保存当前最好的模型====\")\n",
    "                        checkpoint = {\n",
    "                            'epoch': epoch,\n",
    "                            'loss': dev_loss,\n",
    "                            'state_dict': self.model.state_dict(),\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                        }\n",
    "                        best_dev_micro_f1 = macro_f1\n",
    "                        checkpoint_path = os.path.join(\n",
    "                            self.args.output_dir, 'best.pt')\n",
    "                        self.save_ckp(checkpoint, checkpoint_path)\n",
    "\n",
    "    def dev(self):\n",
    "        # 模型进入评估模式\n",
    "        self.model.eval()\n",
    "\n",
    "        # 记录总的损失\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # 存储预测结果和真实结果\n",
    "        dev_outputs = []\n",
    "        dev_targets = []\n",
    "        with torch.no_grad():\n",
    "            # 便利dev数据\n",
    "            for dev_step, dev_data in enumerate(self.dev_loader):\n",
    "                # 把数据转移到gpu或cpu\n",
    "                token_ids = dev_data['token_ids'].to(self.device)\n",
    "                attention_masks = dev_data['attention_masks'].to(self.device)\n",
    "                token_type_ids = dev_data['token_type_ids'].to(self.device)\n",
    "                labels = dev_data['labels'].to(self.device)\n",
    "\n",
    "                # 获取模型输出\n",
    "                outputs = self.model(\n",
    "                    token_ids, attention_masks, token_type_ids)\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                # val_loss = val_loss + ((1 / (dev_step + 1))) * (loss.item() - val_loss)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # 阈值超过0.6才有输出\n",
    "                outputs = torch.sigmoid(\n",
    "                    outputs).cpu().detach().numpy().tolist()\n",
    "                outputs = (np.array(outputs) > 0.6).astype(int)\n",
    "\n",
    "                # 把本批的结果存储起来\n",
    "                dev_outputs.extend(outputs.tolist())\n",
    "                dev_targets.extend(labels.cpu().detach().numpy().tolist())\n",
    "\n",
    "        return total_loss, dev_outputs, dev_targets\n",
    "\n",
    "    def test(self, checkpoint_path):\n",
    "        # 这个和前面的dev函数大体上差不多\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "        model, optimizer, epoch, loss = self.load_ckp(\n",
    "            model, optimizer, checkpoint_path)\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "        total_loss = 0.0\n",
    "        test_outputs = []\n",
    "        test_targets = []\n",
    "        with torch.no_grad():\n",
    "            for test_step, test_data in enumerate(self.test_loader):\n",
    "                token_ids = test_data['token_ids'].to(self.device)\n",
    "                attention_masks = test_data['attention_masks'].to(self.device)\n",
    "                token_type_ids = test_data['token_type_ids'].to(self.device)\n",
    "                labels = test_data['labels'].to(self.device)\n",
    "                outputs = model(token_ids, attention_masks, token_type_ids)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                # val_loss = val_loss + ((1 / (dev_step + 1))) * (loss.item() - val_loss)\n",
    "                total_loss += loss.item()\n",
    "                outputs = torch.sigmoid(\n",
    "                    outputs).cpu().detach().numpy().tolist()\n",
    "                outputs = (np.array(outputs) > 0.6).astype(\n",
    "                    int)  # 这里允许多分类的存在，即一个句子对应多个事件\n",
    "                test_outputs.extend(outputs.tolist())\n",
    "                test_targets.extend(labels.cpu().detach().numpy().tolist())\n",
    "\n",
    "        return total_loss, test_outputs, test_targets\n",
    "\n",
    "    def predict(self, tokenizer, text, id2label, args):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "        checkpoint = os.path.join(args.output_dir, 'best.pt')\n",
    "        model, optimizer, epoch, loss = self.load_ckp(\n",
    "            model, optimizer, checkpoint)\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer.encode_plus(text=text,\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=args.max_seq_len,\n",
    "                                           truncation='longest_first',\n",
    "                                           padding=\"max_length\",\n",
    "                                           return_token_type_ids=True,\n",
    "                                           return_attention_mask=True,\n",
    "                                           return_tensors='pt')\n",
    "            token_ids = inputs['input_ids'].to(self.device)\n",
    "            attention_masks = inputs['attention_mask'].to(self.device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(self.device)\n",
    "            outputs = model(token_ids, attention_masks, token_type_ids)\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n",
    "            outputs = (np.array(outputs) > 0.6).astype(int)\n",
    "            outputs = np.where(outputs[0] == 1)[0].tolist()\n",
    "            if len(outputs) != 0:\n",
    "                outputs = [id2label[i] for i in outputs]\n",
    "                return outputs\n",
    "            else:\n",
    "                return '不好意思，我没有识别出来'\n",
    "\n",
    "    def get_metrics(self, outputs, targets):\n",
    "        accuracy = accuracy_score(targets, outputs)\n",
    "        micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "        macro_f1 = f1_score(targets, outputs, average='macro')\n",
    "        return accuracy, micro_f1, macro_f1\n",
    "\n",
    "    def get_classification_report(self, outputs, targets, labels):\n",
    "        # confusion_matrix = multilabel_confusion_matrix(targets, outputs)\n",
    "        report = classification_report(targets, outputs, target_names=labels)\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert 11958 examples to features\n",
      "Build 11958 features\n",
      "Convert 1498 examples to features\n",
      "Build 1498 features\n",
      "Some weights of the model checkpoint at C:/Users/ji/.cache/huggingface/hub/models--bert-base-chinese/snapshots/8d2a91f91cc38c96bb8b4556ba70c392f8d5ee55 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 读取配置参数，设置随机数种子和日志记录\n",
    "utils.utils.set_seed(args.seed)\n",
    "utils.utils.set_logger(os.path.join(args.log_dir, 'main.log'))\n",
    "\n",
    "processor = preprocess.Processor()\n",
    "\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "with open('./data/final_data/labels.txt', 'r', encoding='utf-8') as fp:\n",
    "    labels = fp.read().strip().split('\\n')\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "# print(111, label2id)\n",
    "# print(222, label2id)\n",
    "# 这里一共是64类事件\n",
    "# 111 {'财经/交易-出售/收购': 0, '财经/交易-跌停': 1, '财经/交易-加息': 2, '财经/交易-降价': 3, '财经/交易-降息': 4, '财经/交易-融资': 5, '财经/交易-上市': 6, '财经/交易-涨价': 7, '财经/交易-涨停': 8, '产品行为-发布': 9, '产品行为-获奖': 10, '产品行为-上映': 11, '产品行为-下架': 12, ' 产品行为-召回': 13, '交往-道歉': 14, '交往-点赞': 15, '交往-感谢': 16, '交往-会见': 17, '交往-探班': 18, '竞赛行为-夺冠': 19, '竞赛行为-晋级': 20, '竞赛行为-禁赛': 21, '竞赛行为-胜负': 22, '竞赛行为-退赛': 23, '竞赛行为-退役': 24, '人生-产子/女': 25, '人生-出轨': 26, '人生-订婚': 27, '人生-分手': 28, '人生-怀孕': 29, '人生-婚礼': 30, '人生-结婚': 31, '人生-离婚': 32, '人生-庆生': 33, '人生-求婚': 34, '人生-失联': 35, '人生-死亡': 36, '司法行为-罚款': 37, '司法行为-拘捕': 38, '司法行为-举报': 39, '司法行为-开庭': 40, '司法行为-立案': 41, '司法行为-起诉': 42, '司 法行为-入狱': 43, '司法行为-约谈': 44, '灾害/意外-爆炸': 45, '灾害/意外-车祸': 46, '灾害/意外-地震': 47, '灾害/意外-洪灾': 48, '灾害/意外-起火': 49, '灾害/意外-坍/垮塌': 50, '灾害/意外-袭击': 51, '灾害/意外-坠机': 52, '组织关系-裁员': 53, '组织关系-辞/离职': 54, '组织关系-加盟': 55, '组织关系-解雇': 56, '组织关系-解散': 57, '组织关系-解约': 58, '组织关系-停职': 59, '组织关系-退出': 60, '组织行为-罢工': 61, '组织行为-闭幕': 62, '组织行为-开幕': 63, '组织行为-游行': 64}\n",
    "# 222 {'财经/交易-出售/收购': 0, '财经/交易-跌停': 1, '财经/交易-加息': 2, '财经/交易-降价': 3, '财经/交易-降息': 4, '财经/交易-融资': 5, '财经/交易-上市': 6, '财经/交易-涨价': 7, '财经/交易-涨停': 8, '产品行为-发布': 9, '产品行为-获奖': 10, '产品行为-上映': 11, '产品行为-下架': 12, ' 产品行为-召回': 13, '交往-道歉': 14, '交往-点赞': 15, '交往-感谢': 16, '交往-会见': 17, '交往-探班': 18, '竞赛行为-夺冠': 19, '竞赛行为-晋级': 20, '竞赛行为-禁赛': 21, '竞赛行为-胜负': 22, '竞赛行为-退赛': 23, '竞赛行为-退役': 24, '人生-产子/女': 25, '人生-出轨': 26, '人生-订婚': 27, '人生-分手': 28, '人生-怀孕': 29, '人生-婚礼': 30, '人生-结婚': 31, '人生-离婚': 32, '人生-庆生': 33, '人生-求婚': 34, '人生-失联': 35, '人生-死亡': 36, '司法行为-罚款': 37, '司法行为-拘捕': 38, '司法行为-举报': 39, '司法行为-开庭': 40, '司法行为-立案': 41, '司法行为-起诉': 42, '司 法行为-入狱': 43, '司法行为-约谈': 44, '灾害/意外-爆炸': 45, '灾害/意外-车祸': 46, '灾害/意外-地震': 47, '灾害/意外-洪灾': 48, '灾害/意外-起火': 49, '灾害/意外-坍/垮塌': 50, '灾害/意外-袭击': 51, '灾害/意外-坠机': 52, '组织关系-裁员': 53, '组织关系-辞/离职': 54, '组织关系-加盟': 55, '组织关系-解雇': 56, '组织关系-解散': 57, '组织关系-解约': 58, '组织关系-停职': 59, '组织关系-退出': 60, '组织行为-罢工': 61, '组织行为-闭幕': 62, '组织行为-开幕': 63, '组织行为-游行': 64}\n",
    "# 训练数据\n",
    "train_out = preprocess.get_out(\n",
    "    processor, './data/raw_data/train.json', args, label2id, 'train')\n",
    "train_features, train_callback_info = train_out\n",
    "train_dataset = dataset.MLDataset(train_features)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=args.train_batch_size,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=2)\n",
    "\n",
    "# 验证数据\n",
    "dev_out = preprocess.get_out(\n",
    "    processor, './data/raw_data/dev.json', args, label2id, 'dev')\n",
    "dev_features, dev_callback_info = dev_out\n",
    "dev_dataset = dataset.MLDataset(dev_features)\n",
    "dev_loader = DataLoader(dataset=dev_dataset,\n",
    "                        batch_size=args.eval_batch_size,\n",
    "                        num_workers=2)\n",
    "\n",
    "# 开始训练、评估之类的流程\n",
    "trainer = Trainer(args, train_loader, dev_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "【train】 epoch：0 step:0/14960 loss：0.703281\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "logger.info('========进行测试========')\n",
    "checkpoint_path = './checkpoints/best.pt'\n",
    "total_loss, test_outputs, test_targets = trainer.test(checkpoint_path) # 主要是这里\n",
    "accuracy, micro_f1, macro_f1 = trainer.get_metrics(\n",
    "    test_outputs, test_targets)\n",
    "logger.info(\"【test】 loss：{:.6f} accuracy：{:.4f} micro_f1：{:.4f} macro_f1：{:.4f}\".format(\n",
    "    total_loss, accuracy, micro_f1, macro_f1))\n",
    "report = trainer.get_classification_report(\n",
    "    test_outputs, test_targets, labels)\n",
    "logger.info(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测1，测试多个案例\n",
    "trainer = Trainer(args, None, None, None)\n",
    "checkpoint_path = './checkpoints/best.pt'\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_dir)\n",
    "with open(os.path.join('./data/raw_data/test1.json'), 'r', encoding='utf-8') as fp:\n",
    "    lines = fp.read().strip().split('\\n')[:10]  # 批量预测前10条\n",
    "    for line in lines:\n",
    "        text = eval(line)['text']\n",
    "        print(text)\n",
    "        result = trainer.predict(tokenizer, text, id2label, args)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试2，预测单条\n",
    "text = '2019年，习近平主席热情称赞了国家图书馆工作人员的辛勤付出'\n",
    "print(trainer.predict(tokenizer, text, id2label, args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
